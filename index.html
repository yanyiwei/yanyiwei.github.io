<!DOCTYPE HTML>
<!--
	Miniport by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Yiwei Yang</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Nav -->
			<nav id="nav">
				<ul class="container">
					<li><a href="#home">Home</a></li>
					<!-- <li><a href="#research">Research</a></li> -->
					<!-- <li><a href="#project">Projects</a></li> -->
					<li><a href="#contact">Contact</a></li>
					<li><a href="papers/yiwei_cv.pdf">CV</a></li>
				</ul>
			</nav>
			


		<!-- Home -->
			<article id="home" class="wrapper style1">
				<div class="container">
					<div class="row">
						<div class="col-4 col-5-large col-12-medium">
							<span class="image fit"><img src="images/yiwei_at_dude.jpg" alt="" style="border-radius: 16px;"/></span>
						</div>
						<div class="col-8 col-7-large col-12-medium">
							<header>
								<h1>Hi. I'm <strong>Yiwei Yang</strong>.</h1>
							</header>
							<p>I am a 4th year PhD student studying <b>Information Science</b> at the <b> University of Washington</b>, advised by <b>Bill Howe</b>. I also work closely with <b>Aylin Caliskan</b>. I received my undergraduate degree in Computer Science in 2019 from the University of Michigan.</p>
							<p>My research interest lies broadly in <b>fairness</b> in machine learning. Currently, I am working on <b>mitigating biases</b> of Large Language Models. Recently, I worked on improving <b>robustness</b> to <b>spurious correlations</b> using out-of-distribution examples.

							<h3>News</h3>
							<ul>
								<li>
									<b>Y. Yang</b>, A. Liu, R. Wolfe, A. Caliskan, B. Howe. <b>Regularizing Model Gradients with Concepts to Improve Robustness to Spurious Correlations</b> <i>Accepted to ICML SCIS 2023</i>
								</li>
								<li>
									R. Wolfe, <b>Y. Yang</b>, B. Howe, A. Caliskan. <b>Contrastive Language-Vision AI Models Pretrained on Web-Scraped Multimodal Data Exhibit Sexual Objectification Bias</b> <i>Accepted to FAccT 2023</i> Preprint https://arxiv.org/abs/2212.11261
								</li>				
							</ul>
							<!-- <h3>Incoming</h3>
							<ul>
								<li>
									<b>Y. Yang</b>, A. Liu, R. Wolfe, A. Caliskan, B. Howe. <b>Label-Efficient Group Robustness via Out-of-Distribution Concept Curation</b> <i>In Submission to CVPR 2024</i>
								</li>
								<li>
									<b>Y. Yang</b>, W. Thong, A. Xiang. <b>A Fair Teacher Matters in Knowledge Distillation</b> <i>In Submission to ICASSP 2024</i> 
								</li>				
							</ul> -->
						</div>
					</div>
				</div>
			</article>

		<!-- Work -->
			<!-- <article id="research" class="wrapper style2">
				<div class="container">
					<header>
						<h2>Research</h2>
					</header>
					<div class="row aln-center">
						<div class="col-12 col-12-medium col-12-small">
							<section class="box style1">
								<h3>Qlarify: Generating User-Efficient Clarification Questions for Conversational Systems</h3>
								<p>Background: Voice User Interfaces (VUIs) allow users to access information in hands-free and eye-free conditions (e.g. driving, cooking). People often make ambiguous references when making a request to VUIs, which causes agents to ask follow-up clarification questions. In situations when users divide attention across multiple tasks, answering each additional question adds an expense to user's cognitive effort. This leads to the problem of asking informative and easy-to-answer questions that require users less effort to clarify the ambiguity. </p>
								<p>Approach: We introduce a hybrid human-machine approach to generate questions that balance the trade-off between information gain and answerability. Given a list of questions generated from attributes of the ambiguous reference (e.g. genre of a movie, which can be obtained through Web APIs) and a set of possible reference candidates (e.g. all movies on web), we leverage machine intelligence to rank questions by information gain (i.e. conditional entropy). Then, we leverage crowd intelligence to estimate the perceived effort to answer each question (i.e. how difficult is it to recall and describe), and vote on a question that best balances the trade-off.</p>
							</section>
						</div>
						<div class="col-12 col-12-medium col-12-small">
							<section class="box style1">
								<h3>HEIDL: Learning Linguistic Expressions with Deep Learning and Human-in-the-Loop</h3>
								<p>Background: Prior work in human-in-the-loop machine learning methods has focused on eliciting knowledge from people to create more powerful models. For example, active learning samples examples and solicits people for labels to reduce the overall labeling effort. However, merely asking for labels is a limited use of human knowledge. People are able to not only provide labels to single examples, but also identify rules that label examples in batch.  </p>
								<p>Approach: We train a deep neural network to suggest first-order-logic rules. Each rule classifies a batch of examples to a label. HEIDL presents the learned rules to NLP engineers, and helps them examine, understand, and select a trusted set of rules that generalize to real world cases. By learning rules, we enable a novel interaction paradigm between people and the learned model, allowing people to directly modify the model logic (rules) rather than model predictions (sampled examples). </p>
							</section>
						</div>
					</div>
				</div>
			</article> -->

		<!-- Portfolio -->
			<!-- <article id="project" class="wrapper style1" >
				<div class="container">
					<header>
						<h2>Projects</h2>
					</header>
					<div class="row">

						<div class="col-12-medium">
							<h2>Improving Robustness to Spurious Correlations via Concepts</h2>
							<p>We propose Concept Correction, a framework for correcting spurious attributes in biased models using concepts, sets
								of examples representing some pattern. we show an example from the waterbirds dataset, where (A) directly training an ERM results in
								a model that spuriously correlates background type with bird type. A practitioner can correct this by (B) finding images with land and water
								backgrounds and using Concept Correction. Notably, these example images can be obtained outside of the original waterbirds datset, such
								as using Generative AI, and using search engines.</p>

							<span class="image"><img class="image" src="images/fig1.png" alt="" style="border-radius: 16px;"></span>


						</div>

			

					</div>
				</div>
			</article> -->

		<!-- Contact -->
			<article id="contact" class="wrapper style4">
				<div class="container medium">
					<header>
						<h2>Contact</h2>
					</header>
					<div class="row">
						<div class="col-12">
							<p>
								Email: yanyiwei@uw.edu 
							</p>
							
						</div>
					</div>
					<footer>
						<ul id="copyright">
							<li>&copy; Untitled. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
						</ul>
					</footer>
				</div>
			</article>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>